{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "x_values = ['a10a3f92e9--item--hwrM1', 'a10a3f92e9--item--iWTsg', 'a10a3f92e9--item--qJhdR']\n",
    "columns_to_convert_bin = ['Холодильник', 'Посудомоечная машина', 'Стиральная машина', 'Кондиционер', 'Телевизор', 'Интернет', 'Мебель на кухне', 'Мебель в комнатах']\n",
    "columns = ['Холодильник', 'Посудомоечная машина', 'Стиральная машина', 'Кондиционер', 'Телевизор', 'Интернет',\n",
    "           'Мебель на кухне', 'Мебель в комнатах', 'Оплата ЖКХ', 'Залог', 'Комиссии', 'Предоплата', 'Срок аренды',\n",
    "           'Условия проживания', 'Общая площадь', 'Жилая площадь', 'Площадь кухни', 'Санузел', 'Балкон/лоджия',\n",
    "           'Вид из окон', 'Ремонт', 'Год постройки', 'Количество лифтов', 'Тип перекрытий', 'Парковка',\n",
    "           'Отопление', 'Аварийность', 'link'] # как-то повелся такой порядок, потом нужно просто поставить link в начало\n",
    "urls = pd.read_csv('файл_со_ссылками.csv')['add_link'][:] # слайсы ссылок\n",
    "\n",
    "def convert_to_binary(value):\n",
    "    return 1 if value else 0\n",
    "\n",
    "def remove_column_names_from_values(df, column_names):\n",
    "    for column_name in column_names:\n",
    "        if column_name in df.columns:\n",
    "            df[column_name] = df[column_name].apply(lambda x: x.replace(column_name, '').strip() if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "def process_links(urls, x_values, columns, columns_to_convert_bin, save_interval=20): # основная функция\n",
    "    all_data = []\n",
    "    error_list = []\n",
    "    \n",
    "    for index, url in enumerate(urls):\n",
    "        try:\n",
    "            \n",
    "            response = requests.get(url) # Получение HTML\n",
    "            time.sleep(random.uniform(18, 23))# Здесь и далее подобраны значения для минимального количества ошибок от количества запросов\n",
    "            response.raise_for_status()\n",
    "            html = response.text\n",
    "            \n",
    "            \n",
    "            soup = BeautifulSoup(html, 'html.parser') # Извлечение характеристик\n",
    "            features = []\n",
    "            for x in x_values:\n",
    "                for i in soup.find_all('div', class_=x):\n",
    "                    features.append(i.text.strip())\n",
    "            \n",
    "            \n",
    "            row = {column: None for column in columns}\n",
    "            for feature in features: # Распаковка данных\n",
    "                for column in columns:\n",
    "                    if column in feature:\n",
    "                        extracted_value = re.sub(rf'{column}.*?(\\d+)', r'\\1', feature)\n",
    "                        row[column] = extracted_value.strip()\n",
    "                        break\n",
    "            row['url'] = url\n",
    "            all_data.append(row)\n",
    "            \n",
    "            print(f'Обработано: {index+1} {url}')\n",
    "            \n",
    "            \n",
    "            if (index + 1) % save_interval == 0:# Сохранение каждые save_interval ссылок\n",
    "                save_data(all_data, error_list, columns_to_convert_bin)\n",
    "                all_data.clear()\n",
    "                time.sleep(random.uniform(60, 100))\n",
    "            if (index + 1) % 35 == 0:\n",
    "                time.sleep(random.uniform(300, 600))\n",
    "                \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f'Ошибка при обработке {index+1} {url}: {e}')\n",
    "            error_list.append({'url': url, 'error': str(e)})\n",
    "    \n",
    "    if all_data:\n",
    "        save_data(all_data, error_list, columns_to_convert_bin)\n",
    "\n",
    "def save_data(data, errors, columns_to_convert_bin):\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    for column in columns_to_convert_bin:\n",
    "        df[column] = df[column].apply(convert_to_binary)\n",
    "\n",
    "    text_columns = [col for col in df.columns if col not in columns_to_convert_bin + ['url']]\n",
    "    df = remove_column_names_from_values(df, text_columns)\n",
    "    \n",
    "    df.to_csv('processed_data.csv', mode='a', index=False, header=not bool(pd.read_csv('processed_data.csv').shape[0]))\n",
    "    \n",
    "    if errors:\n",
    "        df_errors = pd.DataFrame(errors)\n",
    "        df_errors.to_csv('errors.csv', mode='a', index=False, header=not bool(pd.read_csv('errors.csv').shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_links(urls, x_values, columns, columns_to_convert_bin, save_interval=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
